{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06RlIxWnThr3"
   },
   "source": [
    "# Deep Learning models\n",
    "## ELMo and GloVe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htiNBmvBwDGV"
   },
   "source": [
    "## Imports and environment configurations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should run this notebook after installing all the requirements in requirements.txt `pip install -r requirements.txt` .\n",
    "\n",
    "It is to note that this notebook was run as a google colab notebook and we took advantage of the GPU offered by this service.\n",
    "In addition some magic commands like `%tensorflow_version 1.x` are only available on colab and it's the equivalent of `!pip install tensorflow==1.15.2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htiNBmvBwDGV"
   },
   "source": [
    "## Imports and environment configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9301,
     "status": "ok",
     "timestamp": 1607651667696,
     "user": {
      "displayName": "Ghribi nour",
      "photoUrl": "https://lh3.googleusercontent.com/-sF0zhzcPhXY/AAAAAAAAAAI/AAAAAAAAUw8/ANSSoVZye6o/s64/photo.jpg",
      "userId": "10556054115865158287"
     },
     "user_tz": -60
    },
    "id": "a0h5gI1lenWR",
    "outputId": "ee7ed1f2-68cd-4356-d6f3-b0726cbdea07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#%tensorflow_version 1.x #Colab magic command\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import regularizers\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "import keras.layers as layers\n",
    "from keras.models import Model, load_model\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Input, Lambda, Dense, Concatenate, GaussianNoise, Bidirectional, LSTM , Flatten, Permute, RepeatVector\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reader(file_path):\n",
    "        x = list(open(file_path, \"r\", encoding='utf-8').readlines())\n",
    "        x = [s.strip() for s in x]\n",
    "        tweets = []\n",
    "        for elem in x:\n",
    "            if elem!='':\n",
    "                tweet=''\n",
    "                for word in elem.split(','):\n",
    "                    tweet+=word+' '\n",
    "                tweets.append(tweet)\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cX_20GZgepuL"
   },
   "outputs": [],
   "source": [
    "x_pos = clean_reader(\"../cleaned_data/cleaned_train_pos.txt\")\n",
    "x_neg = clean_reader(\"../cleaned_data/cleaned_train_neg.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7853,
     "status": "ok",
     "timestamp": 1607634549354,
     "user": {
      "displayName": "Ghribi nour",
      "photoUrl": "https://lh3.googleusercontent.com/-sF0zhzcPhXY/AAAAAAAAAAI/AAAAAAAAUw8/ANSSoVZye6o/s64/photo.jpg",
      "userId": "10556054115865158287"
     },
     "user_tz": -60
    },
    "id": "Z9qHDlBFe-UP",
    "outputId": "f64aee33-8b8d-4143-959b-7e129d1ad31d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88699 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=100000)\n",
    "\n",
    "tokenizer.fit_on_texts(x_pos)\n",
    "tokenizer.fit_on_texts(x_neg)\n",
    "\n",
    "sequences_pos = tokenizer.texts_to_sequences(x_pos)\n",
    "sequences_neg = tokenizer.texts_to_sequences(x_neg)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ARihVzNfKJe"
   },
   "outputs": [],
   "source": [
    "max_pos = max(len(l) for l in sequences_pos)\n",
    "max_neg = max(len(l) for l in sequences_neg)\n",
    "max_ = max(max_pos,max_neg)\n",
    "# padding\n",
    "data_pos = pad_sequences(sequences_pos, maxlen=max_,padding='post',)\n",
    "data_neg = pad_sequences(sequences_neg, maxlen=max_,padding='post',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDRaV3nLfPQO"
   },
   "outputs": [],
   "source": [
    "labels_pos = np.ones(len(data_pos))\n",
    "labels_val = np.zeros(len(data_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8k071F1f9vF"
   },
   "source": [
    "### GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41456,
     "status": "ok",
     "timestamp": 1607634648163,
     "user": {
      "displayName": "Ghribi nour",
      "photoUrl": "https://lh3.googleusercontent.com/-sF0zhzcPhXY/AAAAAAAAAAI/AAAAAAAAUw8/ANSSoVZye6o/s64/photo.jpg",
      "userId": "10556054115865158287"
     },
     "user_tz": -60
    },
    "id": "rSQr9qCcfS4I",
    "outputId": "fbb92562-4120-488a-e18d-0c5070629939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../Resources/glove.6B.300d.txt',encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-65C6OFftgb"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2stvEoaCfwSe"
   },
   "source": [
    "- We load this embedding matrix into an Embedding layer and we get our GLOVE_EMBEDDING_LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3qqusJwfrkD"
   },
   "outputs": [],
   "source": [
    "glove_embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            #input_length=max_,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNjweJa7gHFO"
   },
   "source": [
    "### ELMo embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zi8Fgbq7f31q"
   },
   "source": [
    "- Now we implement elmo embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OobPPQ9dgRcT"
   },
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-Tiplafgf48"
   },
   "outputs": [],
   "source": [
    "x_pos = clean_reader(\"../cleaned_data/cleaned_train_pos.txt\")\n",
    "x_neg = clean_reader(\"../cleaned_data/cleaned_train_neg.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdUtRH9OgrVy"
   },
   "outputs": [],
   "source": [
    "x_poss = list(open(\"../cleaned_data/cleaned_train_pos.txt\", \"r\", encoding='utf-8').readlines())\n",
    "x_poss = [s.strip() for s in x_poss]\n",
    "\n",
    "x_negg = list(open(\"../cleaned_data/cleaned_train_neg.txt\", \"r\", encoding='utf-8').readlines())\n",
    "x_negg = [s.strip() for s in x_negg]\n",
    "\n",
    "lengths_x_pos = []\n",
    "for elem in x_poss:\n",
    "    lengths_x_pos.append(len(elem.split(',')))\n",
    "\n",
    "lengths_x_neg = []\n",
    "for elem in x_negg:\n",
    "    lengths_x_neg.append(len(elem.split(',')))\n",
    "\n",
    "x_pos_tok = []\n",
    "for elem in x_poss:\n",
    "    x_pos_tok.append(elem.split(','))\n",
    "\n",
    "x_neg_tok = []\n",
    "for elem in x_negg:\n",
    "    x_neg_tok.append(elem.split(','))\n",
    "\n",
    "for elem in x_pos_tok:\n",
    "    while(len(elem)!=max_):\n",
    "        elem.append('')\n",
    "\n",
    "for elem in x_neg_tok:\n",
    "    while(len(elem)!=max_):\n",
    "        elem.append('')\n",
    "\n",
    "x_pos_tokenized = []\n",
    "for elem in x_pos_tok:\n",
    "    x_pos_tokenized.append(np.array(elem))\n",
    "\n",
    "x_neg_tokenized = []\n",
    "for elem in x_neg_tok:\n",
    "    x_neg_tokenized.append(np.array(elem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJmiqlfNgz4h"
   },
   "source": [
    "With the tokens signature, the module takes tokenized sentences as input. \n",
    "\n",
    "The input tensor is a string tensor with shape [batch_size, max_length] and an int32 tensor with shape [batch_size] corresponding to the sentence length. \n",
    "\n",
    "The length input is necessary to exclude padding in the case of sentences with varying length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 594,
     "status": "ok",
     "timestamp": 1607635176958,
     "user": {
      "displayName": "Ghribi nour",
      "photoUrl": "https://lh3.googleusercontent.com/-sF0zhzcPhXY/AAAAAAAAAAI/AAAAAAAAUw8/ANSSoVZye6o/s64/photo.jpg",
      "userId": "10556054115865158287"
     },
     "user_tz": -60
    },
    "id": "3V0AY92uhOVP",
    "outputId": "59a434d3-3a0f-40ef-d395-e645e42d1a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)  ##verify:= 1.15.2 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1Ni5-UqhbRk"
   },
   "outputs": [],
   "source": [
    "# we will load the elmo module\n",
    "elmo_module = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n",
    "\n",
    "def ELMoEmbedding3(x):\n",
    "    lengths = K.cast(K.argmax(K.cast(K.equal(x, '--PAD--'), 'uint8')),'int32')\n",
    "    return elmo_module(inputs=dict(tokens=x, sequence_len=lengths),\n",
    "                      as_dict=True,\n",
    "                      signature='tokens',\n",
    "                      )['word_emb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tg1UsriwTwW"
   },
   "source": [
    "### ELMo + GloVe Embedding model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmiNeFZ6iUJK"
   },
   "source": [
    "- define two sets of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2019,
     "status": "ok",
     "timestamp": 1607635346219,
     "user": {
      "displayName": "Ghribi nour",
      "photoUrl": "https://lh3.googleusercontent.com/-sF0zhzcPhXY/AAAAAAAAAAI/AAAAAAAAUw8/ANSSoVZye6o/s64/photo.jpg",
      "userId": "10556054115865158287"
     },
     "user_tz": -60
    },
    "id": "xieroY9bh-gr",
    "outputId": "6703e5e9-fc75-441a-cd01-422275545ae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# create the elmo layer and model\n",
    "input_text = Input(shape=(48,) ,dtype=tf.string)\n",
    "embedding = Lambda(ELMoEmbedding3,output_shape=(48,512))(input_text)\n",
    "elmo = Model(inputs =[input_text],outputs =embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1548,
     "status": "ok",
     "timestamp": 1607635386478,
     "user": {
      "displayName": "Ghribi nour",
      "photoUrl": "https://lh3.googleusercontent.com/-sF0zhzcPhXY/AAAAAAAAAAI/AAAAAAAAUw8/ANSSoVZye6o/s64/photo.jpg",
      "userId": "10556054115865158287"
     },
     "user_tz": -60
    },
    "id": "J70gFxExiOOY",
    "outputId": "1d5af953-d9d5-4ee4-a489-0c06375bba1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "input_glove = Input(shape=(48,) , dtype=\"int32\",)\n",
    "# the second branch opreates on the second input\n",
    "glove_embedding = glove_embedding_layer(input_glove)\n",
    "glove = Model(inputs=input_glove, outputs=glove_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Sk5XM9dib71"
   },
   "outputs": [],
   "source": [
    "# Concatenate the two outputs\n",
    "c = Concatenate()([glove.output, elmo.output])\n",
    "\n",
    "# Apply gaussian nois (prevent from overfitting)\n",
    "c = layers.GaussianNoise(0.3)(c)\n",
    "\n",
    "\n",
    "x = Bidirectional(layers.LSTM(64, return_sequences=True,bias_regularizer=regularizers.l2(1e-4),dropout=0.25))(c)\n",
    "attention = Dense(1, activation='tanh',bias_regularizer=regularizers.l2(1e-4))(x)\n",
    "attention = Flatten()(attention)\n",
    "attention = layers.Activation('softmax')(attention)\n",
    "attention = RepeatVector(64*2)(attention)\n",
    "attention = Permute([2,1])(attention)\n",
    "\n",
    "senti = layers.Multiply()([x,attention])\n",
    "senti = layers.Lambda(lambda xin: K.sum(xin,axis=-2),output_shape=(64*2,))(senti)\n",
    "ourput_layer = layers.Dense(1,activation='sigmoid',bias_regularizer=regularizers.l2(1e-4))(senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1607635472449,
     "user": {
      "displayName": "Ghribi nour",
      "photoUrl": "https://lh3.googleusercontent.com/-sF0zhzcPhXY/AAAAAAAAAAI/AAAAAAAAUw8/ANSSoVZye6o/s64/photo.jpg",
      "userId": "10556054115865158287"
     },
     "user_tz": -60
    },
    "id": "nZ7K5c1DioVd",
    "outputId": "e7f30e9b-6e3b-45d2-e84f-c8edbc5f4d86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 48)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 48)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 48, 300)      26610000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 48, 512)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 48, 812)      0           embedding_1[0][0]                \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNoise (None, 48, 812)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 48, 128)      449024      gaussian_noise_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 48, 1)        129         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 48)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 48)           0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 128, 48)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 48, 128)      0           repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 48, 128)      0           bidirectional_1[0][0]            \n",
      "                                                                 permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 128)          0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         lambda_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 27,059,282\n",
      "Trainable params: 449,282\n",
      "Non-trainable params: 26,610,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# our model will accept the inputs of the two branches and\n",
    "# then output a single value\n",
    "\n",
    "model = Model(inputs=[input_text, input_glove], outputs=ourput_layer)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10684683,
     "status": "ok",
     "timestamp": 1607646205509,
     "user": {
      "displayName": "Ghribi nour",
      "photoUrl": "https://lh3.googleusercontent.com/-sF0zhzcPhXY/AAAAAAAAAAI/AAAAAAAAUw8/ANSSoVZye6o/s64/photo.jpg",
      "userId": "10556054115865158287"
     },
     "user_tz": -60
    },
    "id": "uXKvSwJSiz2S",
    "outputId": "d60162a5-0ef8-4bc0-ae95-62a69cf31ba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 5266s 59ms/step - loss: 0.4643 - accuracy: 0.7740 - val_loss: 0.4003 - val_accuracy: 0.8145\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 5397s 60ms/step - loss: 0.3928 - accuracy: 0.8189 - val_loss: 0.3766 - val_accuracy: 0.8313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f5a0afbb550>"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tokenized = x_pos_tokenized + x_neg_tokenized\n",
    "\n",
    "x_lengths = lengths_x_pos + lengths_x_neg\n",
    "\n",
    "sequences_pos = tokenizer.texts_to_sequences(x_pos)\n",
    "padded_pos = pad_sequences(sequences_pos, maxlen=max_,padding='post',)\n",
    "\n",
    "sequences_neg = tokenizer.texts_to_sequences(x_neg)\n",
    "padded_neg = pad_sequences(sequences_neg, maxlen=max_,padding='post',)\n",
    "\n",
    "x_padded = np.concatenate([padded_pos, padded_neg])\n",
    "\n",
    "y = 100000*[1] + 100000*[0]\n",
    "\n",
    "indices = np.random.permutation(x_padded.shape[0])\n",
    "training_idx, test_idx = indices[:180000], indices[180000:]\n",
    "\n",
    "x_tokenized_train, x_tokenized_test = np.array(x_tokenized)[training_idx,:], np.array(x_tokenized)[test_idx,:]\n",
    "x_lengths_train, x_lengths_test = np.array(x_lengths)[training_idx], np.array(x_lengths)[test_idx]\n",
    "x_padded_train , x_padded_test = x_padded[training_idx,:], x_padded[test_idx,:]\n",
    "y_train, y_test = np.array(y)[training_idx], np.array(y)[test_idx]\n",
    "\n",
    "model.fit([x_tokenized_train[90000:],x_padded_train[90000:]], \n",
    "          y_train[90000:],\n",
    "          validation_data=([x_tokenized_test[10000:],x_padded_test[10000:]], y_test[10000:]),\n",
    "          epochs=2,\n",
    "          batch_size=126, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULMc4Y7Ci9k2"
   },
   "outputs": [],
   "source": [
    "tesst = list(open(\"../cleaned_data/cleaned_test_data.txt\", \"r\", encoding='utf-8').readlines())\n",
    "tesst = [s.strip() for s in tesst]\n",
    "lengths_test = []\n",
    "for elem in tesst:\n",
    "    lengths_test.append(len(elem.split(',')))\n",
    "\n",
    "x_test_tok = []\n",
    "for elem in tesst:\n",
    "    x_test_tok.append(elem.split(','))\n",
    "    \n",
    "for elem in x_test_tok:\n",
    "    while(len(elem)!=48):\n",
    "        elem.append('')\n",
    "\n",
    "x_test_tokenized = []\n",
    "for elem in x_test_tok:\n",
    "    x_test_tokenized.append(np.array(elem))\n",
    "\n",
    "tesst = list(open(\"../cleaned_data/cleaned_test_data.txt\", \"r\", encoding='utf-8').readlines())\n",
    "tesst = [s.strip() for s in tesst]\n",
    "x_test = []\n",
    "for elem in tesst:\n",
    "    if elem!='':\n",
    "        tweet=''\n",
    "        for word in elem.split(','):\n",
    "            tweet+=word+' '\n",
    "        x_test.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nqaac6PU_Ui"
   },
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(x_test)\n",
    "padded_test = pad_sequences(sequences_test, maxlen=max_,padding='post',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wjdah3GAcB3O"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict([x_test_tokenized,padded_test])\n",
    "\n",
    "predictions = []\n",
    "for elem in y_pred:\n",
    "    if(elem[0])<0.5 : x=-1\n",
    "    else: x =1\n",
    "    predictions.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_submission(y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    ids=np.arange(1,10001)\n",
    "    with open(name, 'w',newline='') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkKwElNWXaUe"
   },
   "outputs": [],
   "source": [
    "create_csv_submission(predictions,\"ELMo_GloVe_MODEL.csv\") #0.827 Acc 0.831 F1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NN_Nour.ipynb",
   "provenance": [
    {
     "file_id": "1L3eoymEePxJOiRbvW8WMXHMwGln45MYF",
     "timestamp": 1607187727335
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
