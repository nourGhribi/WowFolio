{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06RlIxWnThr3"
   },
   "source": [
    "# Deep Learning models\n",
    "## XLM_roBERTa model:  (BERT embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was run on a Google cloud Deep Learning VM. We had created a VM with 2vCPUs and 52Go of Memory and an Nvidia Tesla P100 GPU. We recommend as in the other notebooks to run it after installing the requirements `pip install -r requirements.txt` and having a suitable GPU to ensure fast trainng.\n",
    "With the Google Cloud VM it took 7 hours to fit.\n",
    "We used tensorflow, transformers, and pretrained models from https://huggingface.co/.\n",
    "\n",
    "This model Gave us 0.877 accuracy and 0.878 F1-Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant imports and checking environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZ0eE9VByN7N",
    "outputId": "4d58d176-6890-4f21-8ec4-b3ffa5a44330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "npUuVPiyyhW-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bmzkaQK70msO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Let's import a fast tokenizer that can work on batched inputs\n",
    "# (the 'Fast' tokenizers in HuggingFace)\n",
    "from transformers import RobertaTokenizerFast, logging as transformers_logging\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Let's load a pretrained TF2 Roberta model and a simple optimizer\n",
    "from transformers import TFXLMRobertaForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 14 22:06:49 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   57C    P0    35W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check the current GPU infos\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5210981556642462867\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16454849342812487283\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 5025661241410285274\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15703311680\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6551095889574330453\n",
      "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "adNas3RSyiZK"
   },
   "outputs": [],
   "source": [
    "x_poss = list(open(\"Project/cleaned_data/cleaned_train_pos_full.txt\", \"r\", encoding='utf-8').readlines())\n",
    "x_poss = [s.strip() for s in x_poss]\n",
    "x_pos = []\n",
    "for elem in x_poss:\n",
    "    if elem!='':\n",
    "        tweet=''\n",
    "        for word in elem.split(','):\n",
    "            tweet+=word+' '\n",
    "        x_pos.append(tweet)\n",
    "x_negg = list(open(\"Project/cleaned_data/cleaned_train_neg_full.txt\", \"r\", encoding='utf-8').readlines())\n",
    "x_negg = [s.strip() for s in x_negg]\n",
    "x_neg = []\n",
    "for elem in x_negg:\n",
    "    if elem!='':\n",
    "        tweet=''\n",
    "        for word in elem.split(','):\n",
    "            tweet+=word+' '\n",
    "        x_neg.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Z8tH4H0cZwLd"
   },
   "outputs": [],
   "source": [
    "transformers_logging.set_verbosity_warning()\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fSwuQfI8lFiD"
   },
   "outputs": [],
   "source": [
    "transformers_logging.set_verbosity_info()\n",
    "\n",
    "max_length = 512\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ioV2YKjclFiE"
   },
   "outputs": [],
   "source": [
    "def convert_example_to_feature(tweet):\n",
    "        return tokenizer(tweet, add_special_tokens=True,\n",
    "                                    max_length=None,\n",
    "                                    pad_to_max_length=True,\n",
    "                                    return_attention_mask=True,\n",
    "                                    return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kus0hsxlVh1w",
    "outputId": "e7438540-abc9-41cb-c4c8-3613e73268ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "bert_x = convert_example_to_feature((x_pos + x_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9kU5gvJsY2UO"
   },
   "outputs": [],
   "source": [
    "y = np.concatenate([np.ones(len(x_pos)), np.zeros(len(x_neg))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01oS70cNlFiF",
    "outputId": "61145a6e-881e-432c-e274-bf809084ef39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/tf_model.h5 from cache at /home/jupyter/.cache/huggingface/transformers/22fef2e3c5012c1a8f8d7f024e30275dd2925b076abb5131dc3d1068345b6426.d409db346b0c1408865b9785d36744ccb988186626309ae8f995f86511811602.h5\n",
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFXLMRobertaForSequenceClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFXLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFXLMRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model= TFXLMRobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=True)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "YuT-DwrBmXYH"
   },
   "outputs": [],
   "source": [
    "[input_ids,attention_mask] = np.array( [bert_x['input_ids'], bert_x['attention_mask']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "UYTBBsMTeGDu"
   },
   "outputs": [],
   "source": [
    "bert_x = 0 # to optimize memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Yh-lkgEDftWG"
   },
   "outputs": [],
   "source": [
    "attention_mask = attention_mask.astype(np.int8) # to optimize memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "L-Yy5HgA0Xuk"
   },
   "outputs": [],
   "source": [
    "indices = np.random.permutation(input_ids.shape[0])\n",
    "training_idx, test_idx = indices[:2000000], indices[2000000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "COgJ-F-plFiF"
   },
   "outputs": [],
   "source": [
    "train_input_ids = tf.convert_to_tensor(input_ids[training_idx,:])\n",
    "train_att_mask = tf.convert_to_tensor(attention_mask[training_idx,:])\n",
    "y_train = tf.convert_to_tensor(y[training_idx],dtype=tf.float32)\n",
    "\n",
    "\n",
    "test_input_ids = tf.convert_to_tensor(input_ids[test_idx,:])\n",
    "test_att_mask = tf.convert_to_tensor(attention_mask[test_idx,:])\n",
    "y_test = tf.convert_to_tensor(y[test_idx],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ptvWmnoqtxKx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "31250/31250 [==============================] - 30650s 981ms/step - loss: 0.2974 - accuracy: 0.8705 - val_loss: 0.2738 - val_accuracy: 0.8847\n"
     ]
    }
   ],
   "source": [
    "roberta_history = model.fit([train_input_ids,train_att_mask], \n",
    "      y_train,\n",
    "      validation_data=([test_input_ids,test_att_mask], y_test),\n",
    "      epochs=1, batch_size=batch_size, verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./xlm_roberta_weights_1M.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "gdqeJZ1Dy9Q8"
   },
   "outputs": [],
   "source": [
    "tesst = list(open(\"Project/cleaned_data/cleaned_test_data.txt\", \"r\", encoding='utf-8').readlines())\n",
    "tesst = [s.strip() for s in tesst]\n",
    "test = []\n",
    "for elem in tesst:\n",
    "    if elem!='':\n",
    "        tweet=''\n",
    "        for word in elem.split(','):\n",
    "            tweet+=word+' '\n",
    "        test.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfC1LJZz-d1E",
    "outputId": "875e3401-a98f-44ba-d534-fd8ca8d7c6d0"
   },
   "outputs": [],
   "source": [
    "bert_test = convert_example_to_feature(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "vJ10xGWHlFiG"
   },
   "outputs": [],
   "source": [
    "input_ids = tf.convert_to_tensor(bert_test.get('input_ids'))\n",
    "attention_mask =tf.convert_to_tensor(bert_test.get('attention_mask'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "hZamICYClFiG"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict([input_ids,attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cE7_ykt7lFiH",
    "outputId": "5b6f3458-4ce2-4dc2-9cb5-2c7d7dc1835d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.1326149, -3.456763 ], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ArcL8z3clFiH"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "BlfSmj6ulFiI"
   },
   "outputs": [],
   "source": [
    "predictions = [softmax(x) for x in y_pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "4yC3JoFylFiI"
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "for elem in predictions:\n",
    "    if elem[0]>elem[1] : x=-1\n",
    "    else : x = 1\n",
    "    output.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TmZ2upokqBEc",
    "outputId": "9f04fb02-c172-4ce6-d0c6-49d876efbc39"
   },
   "outputs": [],
   "source": [
    "from helpers import create_csv_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "tvanyM7BlFiI"
   },
   "outputs": [],
   "source": [
    "create_csv_submission(output, './xlm_roberta_FULL.csv') # 0.877 ACC 0.878 F1"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
