{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06RlIxWnThr3"
   },
   "source": [
    "# Deep Learning models\n",
    "## BERT embeddings model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was run on a Google cloud Deep Learning VM. We had created a VM with 2vCPUs and 52Go of Memory and an Nvidia Tesla P100 GPU. We recommend as in the other notebooks to run it after installing the requirements `pip install -r requirements.txt` and having a suitable GPU to ensure fast trainng.\n",
    "With the Google Cloud VM it took 7 hours to fit.\n",
    "We used tensorflow, transformers, and pretrained models from https://huggingface.co/.\n",
    "\n",
    "This model was the best model and gave us 0.883 in accuracy and 0.883 F1-Score in AiCrowd.\n",
    "And below you'll see an accuracy of 0.8733 on training and 0.8845 on validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant imports and checking environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZ0eE9VByN7N",
    "outputId": "4d58d176-6890-4f21-8ec4-b3ffa5a44330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 16 14:13:37 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   43C    P0    31W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check the current GPU infos\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10108183292455098698\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 17096325159327525633\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12178093431057456484\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15703311680\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14323153864330370461\n",
      "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "npUuVPiyyhW-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Let's import a fast tokenizer that can work on batched inputs\n",
    "# (the 'Fast' tokenizers in HuggingFace)\n",
    "from transformers import BertTokenizerFast, logging as transformers_logging\n",
    "# Let's load a pretrained TF2 Bert model and a simple optimizer\n",
    "from transformers import TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import create_csv_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "adNas3RSyiZK"
   },
   "outputs": [],
   "source": [
    "x_poss = list(open(\"Project/cleaned_data/cleaned_train_pos_full.txt\", \"r\", encoding='utf-8').readlines())\n",
    "x_poss = [s.strip() for s in x_poss]\n",
    "x_pos = []\n",
    "for elem in x_poss:\n",
    "    if elem!='':\n",
    "        tweet=''\n",
    "        for word in elem.split(','):\n",
    "            tweet+=word+' '\n",
    "        x_pos.append(tweet)\n",
    "x_negg = list(open(\"Project/cleaned_data/cleaned_train_neg_full.txt\", \"r\", encoding='utf-8').readlines())\n",
    "x_negg = [s.strip() for s in x_negg]\n",
    "x_neg = []\n",
    "for elem in x_negg:\n",
    "    if elem!='':\n",
    "        tweet=''\n",
    "        for word in elem.split(','):\n",
    "            tweet+=word+' '\n",
    "        x_neg.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_pos+x_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Z8tH4H0cZwLd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d293b2a7d4614efd93932a792b9a74b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3623ceae0e945eea7be2f12d11e5ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=466062.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transformers_logging.set_verbosity_warning()\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "fSwuQfI8lFiD"
   },
   "outputs": [],
   "source": [
    "#transformers_logging.set_verbosity_info()\n",
    "max_length = 126\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ioV2YKjclFiE"
   },
   "outputs": [],
   "source": [
    "def convert_example_to_feature(tweet):\n",
    "    return tokenizer(tweet, add_special_tokens=True,\n",
    "                                    max_length=None,\n",
    "                                    pad_to_max_length=True,\n",
    "                                    return_attention_mask=True,\n",
    "                                    return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kus0hsxlVh1w",
    "outputId": "e7438540-abc9-41cb-c4c8-3613e73268ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "bert_x = convert_example_to_feature((x_pos+x_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "9kU5gvJsY2UO"
   },
   "outputs": [],
   "source": [
    "y = np.concatenate([np.ones(len(x_pos)), np.zeros(len(x_neg))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01oS70cNlFiF",
    "outputId": "61145a6e-881e-432c-e274-bf809084ef39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier', 'dropout_75']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model= TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=True)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos, x_neg = (0, 0) # optimize memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "YuT-DwrBmXYH"
   },
   "outputs": [],
   "source": [
    "attention_mask = np.array(bert_x['attention_mask'],dtype=np.int8)\n",
    "input_ids= np.array(bert_x['input_ids'],dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "UYTBBsMTeGDu"
   },
   "outputs": [],
   "source": [
    "bert_x = 0 # to optimize memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "L-Yy5HgA0Xuk"
   },
   "outputs": [],
   "source": [
    "indices = np.random.permutation(input_ids.shape[0])\n",
    "training_idx, test_idx = indices[:2000000], indices[2000000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "COgJ-F-plFiF"
   },
   "outputs": [],
   "source": [
    "train_input_ids = tf.convert_to_tensor(input_ids[training_idx,:])\n",
    "train_att_mask = tf.convert_to_tensor(attention_mask[training_idx,:])\n",
    "y_train = tf.convert_to_tensor(y[training_idx],dtype=tf.float32)\n",
    "\n",
    "\n",
    "test_input_ids = tf.convert_to_tensor(input_ids[test_idx,:])\n",
    "test_att_mask = tf.convert_to_tensor(attention_mask[test_idx,:])\n",
    "y_test = tf.convert_to_tensor(y[test_idx],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ptvWmnoqtxKx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31250/31250 [==============================] - 22612s 724ms/step - loss: 0.2904 - accuracy: 0.8733 - val_loss: 0.2704 - val_accuracy: 0.8845\n"
     ]
    }
   ],
   "source": [
    "bert_history = model.fit([train_input_ids,train_att_mask], \n",
    "      y_train,\n",
    "      validation_data=([test_input_ids,test_att_mask], y_test),\n",
    "      epochs=1, batch_size=batch_size, verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./full_training_bert_weights_1M.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "gdqeJZ1Dy9Q8"
   },
   "outputs": [],
   "source": [
    "tesst = list(open(\"Project/cleaned_data/cleaned_test_data.txt\", \"r\", encoding='utf-8').readlines())\n",
    "tesst = [s.strip() for s in tesst]\n",
    "test = []\n",
    "for elem in tesst:\n",
    "    if elem!='':\n",
    "        tweet=''\n",
    "        for word in elem.split(','):\n",
    "            tweet+=word+' '\n",
    "        test.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfC1LJZz-d1E",
    "outputId": "875e3401-a98f-44ba-d534-fd8ca8d7c6d0"
   },
   "outputs": [],
   "source": [
    "bert_test = convert_example_to_feature(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "vJ10xGWHlFiG"
   },
   "outputs": [],
   "source": [
    "input_ids = tf.convert_to_tensor(bert_test.get('input_ids'))\n",
    "attention_mask =tf.convert_to_tensor(bert_test.get('attention_mask'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "hZamICYClFiG"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict([input_ids,attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cE7_ykt7lFiH",
    "outputId": "5b6f3458-4ce2-4dc2-9cb5-2c7d7dc1835d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.8404596, -3.5887604], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "ArcL8z3clFiH"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "BlfSmj6ulFiI"
   },
   "outputs": [],
   "source": [
    "predictions = [softmax(x) for x in y_pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "4yC3JoFylFiI"
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "for elem in predictions:\n",
    "    if elem[0]>elem[1] : x=-1\n",
    "    else : x = 1\n",
    "    output.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TmZ2upokqBEc",
    "outputId": "9f04fb02-c172-4ce6-d0c6-49d876efbc39"
   },
   "outputs": [],
   "source": [
    "from helpers import create_csv_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "tvanyM7BlFiI"
   },
   "outputs": [],
   "source": [
    "create_csv_submission(output, './bert_FULL.csv')  # ACC 0.883 F1-Score 0.883\t"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
