#!/usr/bin/env python
# coding: utf-8

import re
import numpy as np
import pickle
import pandas as pd
from cleaning.data_cleaning import *
import csv

import os.path



import csv
## Helper function that will perform cleaning methods to (full or sample) training data and test data 
## and return cleaned pos_tweets,neg_tweets,test_tweets.
## if load=True then just load the cleaned_data

def load_cleaned_data(file_in,emojis=True, repetitions=True,
                        numbers=True, hashtag=True,
                        apostrophes=False, tokenizing=True,
                        slang=True, spelling=True,
                        punctuations=True, stop_words=True,
                        stemming=True, lemmatizing=True):

    dir_name = os.path.dirname(__file__)
    dir_name = dir_name+'/../'

    test_data= dir_name+file_in
    output_test= dir_name+'cleaned_data/cleaned_data.txt'    


    test = list(open(test_data, "r", encoding='utf-8').readlines())
    test = [s.strip() for s in test]
    test = [x.split(',',1)[1] for x in test]

    # We will have to clean the data
    if repetitions:
        print("Ommiting repetitions")
        test = [ommit_repetitions(tweet) for tweet in test]
        
        
    if emojis:
        print("Translating emojis")
        test = [translate_emoji(tweet) for tweet in test]
    
    if slang:
        print('dealing with slang words')
        test = [deal_slang(tokens) for tokens in test]

    if numbers:
        print("removing numbers")
        test = [remove_numbers(tweet) for tweet in test]
        
    
    if hashtag:
        print("adding <tag> for hashtags")
        test = [add_hashtag(tweet) for tweet in test]
    
    
    if apostrophes:
        print("processing apostrophes")
        test = [apostrophe(tweet) for tweet in test]
    
    if spelling:
        print('correcting spelling mistakes')
        test = [correct_spelling_from_dict(tweet) for tweet in test]
        
    if tokenizing:
        print('tokenizing')
        test = [text_processor.pre_process_doc(tweet) for tweet in test]
    
    
    if punctuations:
        print("removing ponctuations")
        test = remove_punctuations(test)
    

    if slang:
        print('dealing with slang words')
        test = [deal_slang(tokens) for tokens in test]
        
        
    
    if stop_words:
        print("removing stop words")
        test = [remove_stop_words(tweet) for tweet in test]
    
        
    with open(output_test,'w',encoding='utf-8') as f:
        wr = csv.writer(f)
        wr.writerows(test)

    return test


## This will load the created embeddings (after applying cooc.py and glove_template.py) and create a corresponding dataframe
def load_word_embeddings_df(path_embeddings,path_vocab):
    embeddings = np.load(path_embeddings)
    vocab = pickle.load(open(path_vocab, "rb")) #this file was generated by executing vocab.sh
    word_embedding = {}
    for key in vocab.keys():
        word_embedding[key] = embeddings[vocab.get(key)] # keys in this dict are not encoded 
    return pd.DataFrame(word_embedding).T


## A helper function that will calculate average word vectors for each tweet using the word embeddings
def average_word_vectors(tweets ,word_embedding):
    error = 0
    avg_word_vectors = np.zeros((len(tweets), word_embedding.shape[1] ))
    for i, tweet in enumerate(tweets):
        
        split_tweet = tweet.split()
        nb_words = 0
        
        for word in split_tweet:
            try:
                avg_word_vectors[i] += word_embedding.loc[word].to_numpy()
                nb_words += 1

            except KeyError: 
                continue
        if (nb_words != 0):
            avg_word_vectors[i] /= nb_words
        
    return avg_word_vectors



## This function will create a train_df containings both positive and negative tweets, and a test_df.
def create_test_dfs(test):
    test_df = pd.DataFrame({'tweets': test})
    test_df['tweets'] = test_df['tweets'].apply(lambda x: " ".join(x))
    return test_df



def create_csv_submission(y_pred, name):
    """
    Creates an output file in csv format for submission to kaggle
    Arguments: ids (event ids associated with each prediction)
               y_pred (predicted class labels)
               name (string name of .csv output file to be created)
    """
    ids=np.arange(1,10001)
    with open(name, 'w',newline='') as csvfile:
        fieldnames = ['Id', 'Prediction']
        writer = csv.DictWriter(csvfile, delimiter=",", fieldnames=fieldnames)
        writer.writeheader()
        for r1, r2 in zip(ids, y_pred):
            writer.writerow({'Id':int(r1),'Prediction':int(r2)})
